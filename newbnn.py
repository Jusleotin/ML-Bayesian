# -*- coding: utf-8 -*-
"""newbnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MsEpzqRN9o8FRf30yyJzgBSeYpAT6570
"""

# Commented out IPython magic to ensure Python compatibility.
# Imported Libraries

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA, TruncatedSVD
import matplotlib.patches as mpatches
import time

# Classifier Libraries
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

import collections

#Tensor flow tamplate
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_datasets as tfds
import tensorflow_probability as tfp


from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model


# Other Libraries
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline
from sklearn.model_selection import cross_val_score
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import NearMiss
from imblearn.metrics import classification_report_imbalanced
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report, mean_squared_error
from collections import Counter
from sklearn.model_selection import KFold, StratifiedKFold
import warnings
warnings.filterwarnings("ignore")

# %matplotlib inline



# from google.colab import drive
# drive.mount('/content/drive')
# file_path = '/content/drive/My Drive/MLB_Project/creditcard.csv'  #sometimes the path of file may change
file_path = 'creditcard.csv'
data = pd.read_csv(file_path) # Reading the file .csv
type(data)
df = pd.DataFrame(data) # Converting data to Panda DataFrame

# df.shape
# # df.head()

from sklearn.preprocessing import StandardScaler, RobustScaler

# RobustScaler is less prone to outliers.

std_scaler = StandardScaler()
rob_scaler = RobustScaler()

df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))
df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))

df.drop(['Time','Amount'], axis=1, inplace=True)

X = df.drop('Class', axis=1)
y = df['Class']

## Sub-sampling

df = df.sample(frac=1)

# amount of fraud classes 492 rows keep them consistent.
fraud_df = df.loc[df['Class'] == 1]
non_fraud_df = df.loc[df['Class'] == 0][:492]

normal_distributed_df = pd.concat([fraud_df, non_fraud_df])

# Shuffle dataframe rows
new_df = normal_distributed_df.sample(frac=1, random_state=42)

new_df.shape
print('Distribution of the Classes in the subsample dataset')
print(new_df['Class'].value_counts()/len(new_df))

## Split the balanced data into train and test
X = new_df.drop('Class', axis=1)
y = new_df['Class']

#### No NearMiss

from imblearn.under_sampling import NearMiss
from collections import Counter

nm = NearMiss()

# Undersample the majority class
X_resampled, y_resampled = nm.fit_resample(X, y)

print("Class distribution after Near-Miss undersampling:", Counter(y_resampled))

X_resampled = X_resampled.values
y_resampled = y_resampled.values
# X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)


# # Turn the values into an array for feeding the classification algorithms.
# X_train = X_train.values
# X_test = X_test.values
# y_train = y_train.values
# y_test = y_test.values

# dataset = tf.data.Dataset.from_tensor_slices((X_resampled, y_resampled))

def split_features_and_labels(features, label):
    return features, label

def get_train_and_test_splits(train_size, batch_size=1):
    # We prefetch with a buffer the same size as the dataset because th dataset
    # is very small and fits into memory.
    # dataset = dataset.map(split_features_and_labels)
    # dataset = tf.data.Dataset.from_tensor_slices((X_resampled, y_resampled))
    dataset = tf.data.Dataset.from_tensor_slices((X, y))
    dataset = dataset.map(lambda x, y: (x, tf.cast(y, tf.float32)))
    dataset.prefetch(buffer_size=dataset_size)
    dataset.cache()

    train_dataset = (
        dataset.take(train_size).shuffle(buffer_size=train_size).batch(batch_size)
    )
    test_dataset = dataset.skip(train_size).batch(batch_size)

    return train_dataset, test_dataset

FEATURE_NAMES = [
    "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "Time", "Amount",
]


def create_model_inputs():
    inputs = {}
    for feature_name in FEATURE_NAMES:
        inputs[feature_name] = layers.Input(
            name=feature_name, shape=(1,), dtype=tf.float32
        )
    return inputs

hidden_units = [512, 512, 512, 512, 512]
learning_rate = 0.001


def run_experiment(model, loss, train_dataset, test_dataset):

    model.compile(
        optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),
        loss=loss,
        metrics=[keras.metrics.RootMeanSquaredError(),
            keras.metrics.BinaryAccuracy(),
            keras.metrics.Precision(),
            keras.metrics.Recall(),
            keras.metrics.AUC(),],
    )

    print("Start training the model...")
    model.fit(train_dataset, epochs=num_epochs, validation_data=test_dataset)
    print("Model training finished.")

     # Evaluate the model on train dataset
    print("Evaluating model performance on train dataset...")
    train_loss, train_rmse, train_binary_accuracy, train_precision, train_recall, train_auc = model.evaluate(train_dataset, verbose=0)
    print(f"Train Loss: {round(train_loss, 3)}, RMSE: {round(train_rmse, 3)}, Accuracy: {round(train_binary_accuracy, 3)}, Precision: {round(train_precision, 3)}, Recall: {round(train_recall, 3)}, AUC: {round(train_auc, 3)}")

    # Evaluate the model on test dataset
    print("Evaluating model performance on test dataset...")
    test_loss, test_rmse, test_binary_accuracy, test_precision, test_recall, test_auc = model.evaluate(test_dataset, verbose=0)
    print(f"Test Loss: {round(test_loss, 3)}, RMSE: {round(test_rmse, 3)}, Accuracy: {round(test_binary_accuracy, 3)}, Precision: {round(test_precision, 3)}, Recall: {round(test_recall, 3)}, AUC: {round(test_auc, 3)}")

def create_baseline_model():
    inputs = create_model_inputs()
    input_values = [value for _, value in sorted(inputs.items())]
    features = keras.layers.concatenate(input_values)
    features = layers.BatchNormalization()(features)

    # Create hidden layers with deterministic weights using the Dense layer.
    for units in hidden_units:
        features = layers.Dense(units, activation="sigmoid")(features)
    # The output is deterministic: a single point estimate.
    outputs = layers.Dense(units=1)(features)

    model = keras.Model(inputs=inputs, outputs=outputs)
    return model

# dataset_size = 492+284315
dataset_size = 492
batch_size = 256
train_size = int(dataset_size * 0.8)
train_dataset, test_dataset = get_train_and_test_splits(train_size, batch_size)
print("the type of training data is ", type(train_dataset))
print(test_dataset)
# Verify train and test datasets
print("Train dataset size:", len(train_dataset))
print("Test dataset size:", len(test_dataset))

# Check if the datasets are empty
assert len(train_dataset) > 0, "Train dataset is empty"
assert len(test_dataset) > 0, "Test dataset is empty"

def prepare_input_features_and_labels(features, labels):
    return {feature_name: features[:, i] for i, feature_name in enumerate(FEATURE_NAMES)}, labels

train_dataset = train_dataset.map(prepare_input_features_and_labels)
test_dataset = test_dataset.map(prepare_input_features_and_labels)

# Verify non-empty datasets after mapping
assert len(train_dataset) > 0, "Train dataset is empty after mapping"
assert len(test_dataset) > 0, "Test dataset is empty after mapping"

num_epochs = 100
mse_loss = keras.losses.MeanSquaredError()

baseline_model = create_baseline_model()
run_experiment(baseline_model, mse_loss, train_dataset, test_dataset)

sample = 10
examples, targets = list(test_dataset.unbatch().shuffle(batch_size * 10).batch(sample))[
    0
]

predicted = baseline_model(examples).numpy()
for idx in range(sample):
    print(f"Predicted: {round(float(predicted[idx][0]), 1)} - Actual: {targets[idx]}")

# Define the prior weight distribution as Normal of mean=0 and stddev=1.
# Note that, in this example, the we prior distribution is not trainable,
# as we fix its parameters.
def prior(kernel_size, bias_size, dtype=None):
    n = kernel_size + bias_size
    prior_model = keras.Sequential(
        [
            tfp.layers.DistributionLambda(
                lambda t: tfp.distributions.MultivariateNormalDiag(
                    loc=tf.zeros(n), scale_diag=tf.ones(n)
                )
            )
        ]
    )
    return prior_model


# Define variational posterior weight distribution as multivariate Gaussian.
# Note that the learnable parameters for this distribution are the means,
# variances, and covariances.
def posterior(kernel_size, bias_size, dtype=None):
    n = kernel_size + bias_size
    posterior_model = keras.Sequential(
        [
            tfp.layers.VariableLayer(
                tfp.layers.MultivariateNormalTriL.params_size(n), dtype=dtype
            ),
            tfp.layers.MultivariateNormalTriL(n),
        ]
    )
    return posterior_model

def create_bnn_model(train_size):
    inputs = create_model_inputs()
    features = keras.layers.concatenate(list(inputs.values()))
    features = layers.BatchNormalization()(features)

    # Create hidden layers with weight uncertainty using the DenseVariational layer.
    for units in hidden_units:
        features = tfp.layers.DenseVariational(
            units=units,
            make_prior_fn=prior,
            make_posterior_fn=posterior,
            kl_weight=1 / train_size,
            activation="sigmoid",
        )(features)

    # The output is deterministic: a single point estimate.
    outputs = layers.Dense(units=1)(features)
    model = keras.Model(inputs=inputs, outputs=outputs)
    return model

num_epochs = 100
train_sample_size = int(train_size * 0.3)
small_train_dataset = train_dataset.unbatch().take(train_sample_size).batch(batch_size)

bnn_model_small = create_bnn_model(train_sample_size)
run_experiment(bnn_model_small, mse_loss, small_train_dataset, test_dataset)

def compute_predictions(model, iterations=100):
    predicted = []
    for _ in range(iterations):
        predicted.append(model(examples).numpy())
    predicted = np.concatenate(predicted, axis=1)

    prediction_mean = np.mean(predicted, axis=1).tolist()
    prediction_min = np.min(predicted, axis=1).tolist()
    prediction_max = np.max(predicted, axis=1).tolist()
    prediction_range = (np.max(predicted, axis=1) - np.min(predicted, axis=1)).tolist()

    for idx in range(sample):
        print(
            f"Predictions mean: {round(prediction_mean[idx], 2)}, "
            f"min: {round(prediction_min[idx], 2)}, "
            f"max: {round(prediction_max[idx], 2)}, "
            f"range: {round(prediction_range[idx], 2)} - "
            f"Actual: {targets[idx]}"
        )


compute_predictions(bnn_model_small)

num_epochs = 500
bnn_model_full = create_bnn_model(train_size)
run_experiment(bnn_model_full, mse_loss, train_dataset, test_dataset)

compute_predictions(bnn_model_full)

# type(training_score)

y_predict = bnn_model_full.predict(X_test)

# Accuracy
accuracy = accuracy_score(y_test, y_predict)

# Precision
precision = precision_score(y_test, y_predict)

# Recall
recall = recall_score(y_test, y_predict)

# F1-score
f1 = f1_score(y_test, y_predict)

# Mean Squared Error (for regression problems, not classification)
mse = mean_squared_error(y_test, y_predict)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)
print("MSE:", mse)

undersample_predictions = bnn_model_full.predict(original_Xtest)

import itertools
from sklearn.metrics import confusion_matrix

# Create a confusion matrix
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title, fontsize=14)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

undersample_cm = confusion_matrix(original_ytest, undersample_predictions)
actual_cm = confusion_matrix(original_ytest, original_ytest)
labels = ['No Fraud', 'Fraud']

fig = plt.figure(figsize=(16,8))

fig.add_subplot(221)
plot_confusion_matrix(undersample_cm, labels, title="Random UnderSample \n Confusion Matrix", cmap=plt.cm.Blues)

fig.add_subplot(222)
plot_confusion_matrix(actual_cm, labels, title="Confusion Matrix \n (with 100% accuracy)", cmap=plt.cm.Greens)